{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MANA Photosensitizer Property Prediction\n",
        "## Complete Training Pipeline for Google Colab\n",
        "\n",
        "This notebook contains the complete MANA training pipeline, including:\n",
        "1. Google Drive mounting for data/model persistence\n",
        "2. All model architecture code (PaiNN-based GNN)\n",
        "3. Dataset loading and preprocessing\n",
        "4. Training engine with LR scheduling\n",
        "5. Two-phase training: Lambda (pre-training) ‚Üí Phi (fine-tuning)\n",
        "\n",
        "### Prerequisites\n",
        "- Upload your `.h5` dataset files to Google Drive:\n",
        "  - `lambdamax_data.h5`\n",
        "  - `phi_data.h5`\n",
        "- Set the `DRIVE_DATA_PATH` variable below to point to your data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Edit these paths to match your Google Drive structure\n",
        "# ============================================================================\n",
        "\n",
        "# Path to your data folder in Google Drive (relative to /content/drive/MyDrive/)\n",
        "DRIVE_DATA_PATH = \"MANA/data\"\n",
        "\n",
        "# Path where models will be saved in Google Drive\n",
        "DRIVE_MODELS_PATH = \"MANA/models\"\n",
        "\n",
        "# Dataset filenames\n",
        "LAMBDA_DATASET_FILENAME = \"lambdamax_data.h5\"\n",
        "PHI_DATASET_FILENAME = \"phi_data.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Mount Google Drive\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Build full paths\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "DATA_DIR = DRIVE_ROOT / DRIVE_DATA_PATH\n",
        "MODELS_DIR = DRIVE_ROOT / DRIVE_MODELS_PATH\n",
        "\n",
        "LAMBDA_DATASET_PATH = DATA_DIR / LAMBDA_DATASET_FILENAME\n",
        "PHI_DATASET_PATH = DATA_DIR / PHI_DATASET_FILENAME\n",
        "\n",
        "# Create model directories\n",
        "SAVE_DIR_LAMBDA = MODELS_DIR / \"lambda\"\n",
        "SAVE_DIR_PHI = MODELS_DIR / \"phi\"\n",
        "os.makedirs(SAVE_DIR_LAMBDA, exist_ok=True)\n",
        "os.makedirs(SAVE_DIR_PHI, exist_ok=True)\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n",
        "print(f\"Lambda dataset: {LAMBDA_DATASET_PATH}\")\n",
        "print(f\"Phi dataset: {PHI_DATASET_PATH}\")\n",
        "print()\n",
        "print(f\"Lambda dataset exists: {LAMBDA_DATASET_PATH.exists()}\")\n",
        "print(f\"Phi dataset exists: {PHI_DATASET_PATH.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Install Dependencies\n",
        "# ============================================================================\n",
        "!pip install torch-geometric h5py -q\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Model Architecture (MANA with PaiNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MANA Model Architecture\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def scatter_sum(src, index, dim=-1, dim_size=None):\n",
        "    \"\"\"\n",
        "    Native PyTorch implementation of scatter_sum to avoid torch_scatter dependency.\n",
        "    \"\"\"\n",
        "    if dim_size is None:\n",
        "        dim_size = index.max().item() + 1\n",
        "\n",
        "    size = list(src.size())\n",
        "    size[dim] = dim_size\n",
        "    out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
        "\n",
        "    return out.index_add_(dim, index, src)\n",
        "\n",
        "\n",
        "class RadialBasisFunction(nn.Module):\n",
        "    \"\"\"\n",
        "    Module to compute radial basis functions (RBFs) for given distances.\n",
        "    Uses Gaussian RBFs centered at specified points with given widths.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_rbf, cutoff=5.0):\n",
        "        super().__init__()\n",
        "        centers = torch.linspace(0.0, cutoff, num_rbf)\n",
        "        self.register_buffer(\"centers\", centers)\n",
        "        self.gamma = nn.Parameter(torch.ones(num_rbf), requires_grad=False)\n",
        "\n",
        "    def forward(self, distances):\n",
        "        diff = distances.unsqueeze(-1) - self.centers\n",
        "        return torch.exp(-self.gamma * diff**2)\n",
        "\n",
        "\n",
        "class PaiNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the PaiNN architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, num_rbf):\n",
        "        super().__init__()\n",
        "\n",
        "        self.filter_net = nn.Sequential(\n",
        "            nn.Linear(num_rbf, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.update_net = nn.Sequential(\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, s, v, edge_index, edge_attr, rbf):\n",
        "        \"\"\"\n",
        "        PaiNN message passing with strict E(3)-equivariance.\n",
        "\n",
        "        s: (N, F) scalar features\n",
        "        v: (N, F, 3) vector features\n",
        "        edge_index: (2, E)\n",
        "        edge_attr: (E, 4) = (distance, dx, dy, dz)\n",
        "        rbf: (E, num_rbf)\n",
        "        \"\"\"\n",
        "        row, col = edge_index\n",
        "        directions = edge_attr[:, 1:4]  # (E, 3)\n",
        "\n",
        "        phi_ss, phi_vv, phi_sv = self.filter_net(rbf).chunk(3, dim=-1)\n",
        "\n",
        "        m_s = phi_ss * s[col]\n",
        "        m_v = phi_vv.unsqueeze(-1) * v[col] + phi_sv.unsqueeze(\n",
        "            -1\n",
        "        ) * directions.unsqueeze(1) * s[col].unsqueeze(-1)\n",
        "\n",
        "        m_s = scatter_sum(m_s, row, dim=0, dim_size=s.size(0))\n",
        "        m_v = scatter_sum(m_v, row, dim=0, dim_size=v.size(0))\n",
        "\n",
        "        v_norm = torch.norm(m_v, dim=-1)\n",
        "        delta_s, alpha, beta = self.update_net(\n",
        "            torch.cat([s, m_s, v_norm], dim=-1)\n",
        "        ).chunk(3, dim=-1)\n",
        "\n",
        "        s = s + delta_s\n",
        "        v = alpha.unsqueeze(-1) * v + beta.unsqueeze(-1) * m_v\n",
        "\n",
        "        return s, v\n",
        "\n",
        "\n",
        "class LambdaMaxHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts absorption maximum (lambda_max) from molecular embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h_mol):\n",
        "        return self.net(h_mol)\n",
        "\n",
        "\n",
        "class PhiDeltaHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts singlet oxygen quantum yield from molecular embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid(),  # Yield between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, h_mol):\n",
        "        return self.net(h_mol)\n",
        "\n",
        "\n",
        "class MANA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_atom_types,\n",
        "        hidden_dim=128,\n",
        "        num_layers=4,\n",
        "        num_rbf=20,\n",
        "        tasks=None,\n",
        "        lambda_mean=500.0,\n",
        "        lambda_std=100.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if tasks is None:\n",
        "            tasks = [\"lambda\", \"phi\"]\n",
        "        self.tasks = tasks\n",
        "\n",
        "        self.embedding = nn.Embedding(num_atom_types, hidden_dim)\n",
        "        self.rbf = RadialBasisFunction(num_rbf)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [PaiNNLayer(hidden_dim, num_rbf) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        # Lambda head takes only the molecule embedding (128)\n",
        "        self.lambda_head = LambdaMaxHead(hidden_dim)\n",
        "\n",
        "        solvent_dim = 64\n",
        "        # Encodes Dielectric Constant (1 float) -> Vector (64)\n",
        "        self.solvent_encoder = nn.Sequential(\n",
        "            nn.Linear(1, solvent_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(solvent_dim, solvent_dim)\n",
        "        )\n",
        "\n",
        "        # Phi Head takes (Mol_Emb + Solv_Emb) = 128 + 64\n",
        "        self.phi_head = PhiDeltaHead(hidden_dim + solvent_dim)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "        self.register_buffer(\"lambda_mean\", torch.tensor(lambda_mean))\n",
        "        self.register_buffer(\"lambda_std\", torch.tensor(lambda_std))\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, data):\n",
        "        z, edge_index, edge_attr, batch = (\n",
        "            data.x,\n",
        "            data.edge_index,\n",
        "            data.edge_attr,\n",
        "            data.batch,\n",
        "        )\n",
        "\n",
        "        # 1. Run Backbone\n",
        "        dist = edge_attr[:, 0]\n",
        "        rbf = self.rbf(dist)\n",
        "        s = self.embedding(z)\n",
        "        v = torch.zeros(s.size(0), s.size(1), 3, device=s.device)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            s, v = layer(s, v, edge_index, edge_attr, rbf)\n",
        "\n",
        "        # Molecular Embedding (mean pooling)\n",
        "        h_mol = scatter_sum(s, batch, dim=0)\n",
        "        h_mol = h_mol / (torch.bincount(batch).unsqueeze(-1).float() + 1e-9)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # 2. Lambda Head (Standard)\n",
        "        if \"lambda\" in self.tasks:\n",
        "            results[\"lambda\"] = self.lambda_head(h_mol).squeeze(-1)\n",
        "\n",
        "        # 3. Phi Head (Solvent Aware)\n",
        "        if \"phi\" in self.tasks:\n",
        "            if not hasattr(data, 'dielectric'):\n",
        "                raise ValueError(\"Model expects 'data.dielectric' attribute!\")\n",
        "\n",
        "            h_solv = self.solvent_encoder(data.dielectric)\n",
        "\n",
        "            # Concatenate [Molecule, Solvent]\n",
        "            h_combined = torch.cat([h_mol, h_solv], dim=1)\n",
        "            results[\"phi\"] = self.phi_head(h_combined).squeeze(-1)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def loss_fn(self, preds, batch):\n",
        "        \"\"\"\n",
        "        Defines the loss function for training.\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        metrics = {}\n",
        "\n",
        "        if \"lambda\" in self.tasks and hasattr(batch, \"lambda_max\"):\n",
        "            mask = torch.isfinite(batch.lambda_max.squeeze())\n",
        "            if mask.any():\n",
        "                pred_norm = (preds[\"lambda\"][mask] - self.lambda_mean) / self.lambda_std\n",
        "                target_norm = (batch.lambda_max[mask] - self.lambda_mean) / self.lambda_std\n",
        "\n",
        "                loss_lambda = F.huber_loss(pred_norm, target_norm, delta=1.0)\n",
        "                loss += loss_lambda\n",
        "                metrics[\"loss_lambda\"] = loss_lambda.item()\n",
        "\n",
        "        if \"phi\" in self.tasks and hasattr(batch, \"phi_delta\"):\n",
        "            mask = torch.isfinite(batch.phi_delta.squeeze())\n",
        "            if mask.any():\n",
        "                loss_phi = F.mse_loss(preds[\"phi\"][mask], batch.phi_delta[mask])\n",
        "                loss += loss_phi\n",
        "                metrics[\"loss_phi\"] = loss_phi.item()\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "\n",
        "print(\"‚úì MANA model architecture loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Dataset Constructor\n",
        "# ============================================================================\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class GeometricSubset:\n",
        "    \"\"\"\n",
        "    Wrapper to handle train/val/test splits for the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.indices[idx]]\n",
        "\n",
        "\n",
        "class DatasetConstructor(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hdf5_file,\n",
        "        cutoff_radius=5.0,\n",
        "        batch_size=32,\n",
        "        train_split=0.8,\n",
        "        val_split=0.1,\n",
        "        random_seed=42,\n",
        "        num_atom_types=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading raw data from {hdf5_file}...\")\n",
        "        with h5py.File(hdf5_file, \"r\") as f:\n",
        "            self.atomic_numbers = f[\"atomic_numbers\"][()]\n",
        "            self.positions = f[\"geometries\"][()]\n",
        "            self.lambda_max = f[\"lambda_max\"][()]\n",
        "            self.phi_delta = f[\"phi_delta\"][()]\n",
        "            self.mol_ids = f[\"mol_ids\"][()]\n",
        "            self.dielectric = f[\"dielectric\"][()]\n",
        "\n",
        "            raw_smiles = f[\"smiles\"][()]\n",
        "            self.smiles = [\n",
        "                s.decode(\"utf-8\") if isinstance(s, bytes) else s for s in raw_smiles\n",
        "            ]\n",
        "\n",
        "        # 1. Build Vocabulary\n",
        "        unique_atoms = set()\n",
        "        for z in self.atomic_numbers:\n",
        "            unique_atoms.update(z[z > 0])\n",
        "\n",
        "        self.unique_atoms = sorted(list(unique_atoms))\n",
        "        self.atom_to_index = {a: i + 1 for i, a in enumerate(self.unique_atoms)}\n",
        "\n",
        "        self.num_atom_types = (\n",
        "            num_atom_types if num_atom_types is not None else len(self.unique_atoms) + 1\n",
        "        )\n",
        "\n",
        "        self.cutoff_radius = cutoff_radius\n",
        "        self.batch_size = batch_size\n",
        "        self.n_structures = self.atomic_numbers.shape[0]\n",
        "\n",
        "        # 2. PRE-COMPUTE GRAPHS\n",
        "        print(f\"Pre-processing {self.n_structures} molecular graphs...\")\n",
        "        self.data_list = []\n",
        "\n",
        "        for idx in tqdm(range(self.n_structures)):\n",
        "            data_obj = self._process_one(idx)\n",
        "            self.data_list.append(data_obj)\n",
        "\n",
        "        # 3. Create Splits (with full reproducibility)\n",
        "        np.random.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        torch.cuda.manual_seed_all(random_seed)\n",
        "        idx = np.random.permutation(self.n_structures)\n",
        "\n",
        "        n_train = int(train_split * self.n_structures)\n",
        "        n_val = int(val_split * self.n_structures)\n",
        "\n",
        "        self.train_indices = idx[:n_train]\n",
        "        self.val_indices = idx[n_train : n_train + n_val]\n",
        "        self.test_indices = idx[n_train + n_val :]\n",
        "\n",
        "        # Compute normalization stats from training data only\n",
        "        train_lambda = self.lambda_max[self.train_indices]\n",
        "        self.lambda_mean = float(np.nanmean(train_lambda))\n",
        "        self.lambda_std = float(np.nanstd(train_lambda))\n",
        "\n",
        "        print(f\"Data split: {len(self.train_indices)} Train, {len(self.val_indices)} Val, {len(self.test_indices)} Test\")\n",
        "        print(f\"Lambda stats (train): mean={self.lambda_mean:.2f}, std={self.lambda_std:.2f}\")\n",
        "\n",
        "    def _process_one(self, idx):\n",
        "        \"\"\"Internal helper to process a single molecule index into a Data object\"\"\"\n",
        "        z_raw = self.atomic_numbers[idx]\n",
        "\n",
        "        # Map atoms\n",
        "        z = torch.tensor(\n",
        "            [self.atom_to_index[a] if a > 0 else 0 for a in z_raw],\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        pos = torch.tensor(self.positions[idx], dtype=torch.float32)\n",
        "        atom_mask = torch.tensor(z_raw > 0, dtype=torch.bool)\n",
        "\n",
        "        # Squeeze out padding\n",
        "        z = z[atom_mask]\n",
        "        pos = pos[atom_mask]\n",
        "\n",
        "        # Generate Edges\n",
        "        if pos.size(0) == 0:\n",
        "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "            edge_attr = torch.empty((0, 4))\n",
        "        else:\n",
        "            dist = torch.cdist(pos, pos)\n",
        "            mask = (dist < self.cutoff_radius) & (dist > 0)\n",
        "            row, col = mask.nonzero(as_tuple=True)\n",
        "            edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "            diff = pos[col] - pos[row]\n",
        "            d = torch.norm(diff, dim=1, keepdim=True)\n",
        "            u = diff / (d + 1e-8)\n",
        "            edge_attr = torch.cat([d, u], dim=1)\n",
        "\n",
        "        return Data(\n",
        "            x=z,\n",
        "            pos=pos,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr,\n",
        "            lambda_max=torch.tensor([self.lambda_max[idx]], dtype=torch.float32),\n",
        "            phi_delta=torch.tensor([self.phi_delta[idx]], dtype=torch.float32),\n",
        "            mol_id=torch.tensor([self.mol_ids[idx]], dtype=torch.int32),\n",
        "            dielectric=torch.tensor([self.dielectric[idx]], dtype=torch.float32).view(1, 1),\n",
        "            smiles=self.smiles[idx],\n",
        "        )\n",
        "\n",
        "    def len(self):\n",
        "        return self.n_structures\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "    def get_dataloaders(self, num_workers=0):\n",
        "        return (\n",
        "            DataLoader(\n",
        "                GeometricSubset(self, self.train_indices),\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=num_workers,\n",
        "            ),\n",
        "            DataLoader(\n",
        "                GeometricSubset(self, self.val_indices),\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "            ),\n",
        "            DataLoader(\n",
        "                GeometricSubset(self, self.test_indices),\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"‚úì Dataset constructor loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Training Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Training Engine\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim.adam import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use non-interactive backend for Colab\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "\n",
        "class TrainingEngine:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        device,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        hyperparams,\n",
        "        save_dir,\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.max_epochs = hyperparams[\"max_epochs\"]\n",
        "        self.patience = hyperparams[\"early_stopping_patience\"]\n",
        "\n",
        "        self.optimizer = Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=hyperparams[\"learning_rate\"],\n",
        "            weight_decay=hyperparams[\"weight_decay\"],\n",
        "        )\n",
        "\n",
        "        self.scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            factor=0.5,\n",
        "            patience=20,\n",
        "            min_lr=1e-6,\n",
        "        )\n",
        "\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # History for plotting and analysis\n",
        "        self.history = {\n",
        "            \"train_total\": [],\n",
        "            \"val_total\": [],\n",
        "            \"train_lambda\": [],\n",
        "            \"train_phi\": [],\n",
        "            \"val_lambda\": [],\n",
        "            \"val_phi\": [],\n",
        "        }\n",
        "\n",
        "    def train(self):\n",
        "        best_val = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(1, self.max_epochs + 1):\n",
        "            train_total, train_comps = self._train_epoch()\n",
        "            val_total, val_comps = self._validate()\n",
        "\n",
        "            # store history\n",
        "            self.history[\"train_total\"].append(train_total)\n",
        "            self.history[\"val_total\"].append(val_total)\n",
        "\n",
        "            self.history[\"train_lambda\"].append(train_comps.get(\"loss_lambda\", 0))\n",
        "            self.history[\"train_phi\"].append(train_comps.get(\"loss_phi\", 0))\n",
        "            self.history[\"val_lambda\"].append(val_comps.get(\"loss_lambda\", 0))\n",
        "            self.history[\"val_phi\"].append(val_comps.get(\"loss_phi\", 0))\n",
        "\n",
        "            # Step the learning rate scheduler\n",
        "            self.scheduler.step(val_total)\n",
        "\n",
        "            # Print totals and components for transparency\n",
        "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "            lam_str = f\"Œª={train_comps.get('loss_lambda', 0):.2f}\"\n",
        "            phi_str = f\"œÜ={train_comps.get('loss_phi', 0):.4f}\"\n",
        "\n",
        "            val_lam_str = f\"Œª={val_comps.get('loss_lambda', 0):.2f}\"\n",
        "            val_phi_str = f\"œÜ={val_comps.get('loss_phi', 0):.4f}\"\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch:4d} | \"\n",
        "                f\"Train: {train_total:.4f} ({lam_str}, {phi_str}) | \"\n",
        "                f\"Val: {val_total:.4f} ({val_lam_str}, {val_phi_str}) | \"\n",
        "                f\"LR: {current_lr:.2e}\"\n",
        "            )\n",
        "\n",
        "            # checkpointing based on validation total loss\n",
        "            if val_total < best_val:\n",
        "                best_val = val_total\n",
        "                patience_counter = 0\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(self.save_dir, \"best_model.pth\"),\n",
        "                )\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= self.patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # After training, save and plot loss curves\n",
        "        try:\n",
        "            self._save_history()\n",
        "            self._plot_losses()\n",
        "            print(f\"Saved loss history and plots to: {self.save_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to save/plot losses: {e}\")\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        accumulators = {}\n",
        "        n_batches = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for i, batch in enumerate(pbar):\n",
        "            batch = batch.to(self.device)\n",
        "\n",
        "            preds = self.model(batch)\n",
        "\n",
        "            loss, metrics = self.model.loss_fn(preds, batch)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            for k, v in metrics.items():\n",
        "                accumulators[k] = accumulators.get(k, 0.0) + v\n",
        "            n_batches += 1\n",
        "\n",
        "            current_lam = metrics.get(\"loss_lambda\", 0.0)\n",
        "            current_phi = metrics.get(\"loss_phi\", 0.0)\n",
        "            pbar.set_postfix(\n",
        "                {\n",
        "                    \"Loss\": f\"{loss.item():.2f}\",\n",
        "                    \"Œª\": f\"{current_lam:.1f}\",\n",
        "                    \"œÜ\": f\"{current_phi:.4f}\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if n_batches == 0:\n",
        "            return 0.0, {}\n",
        "\n",
        "        avg_metrics = {k: v / n_batches for k, v in accumulators.items()}\n",
        "        return total_loss / n_batches, avg_metrics\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _validate(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        accumulators = {}\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in self.val_loader:\n",
        "            batch = batch.to(self.device)\n",
        "            preds = self.model(batch)\n",
        "            loss, metrics = self.model.loss_fn(preds, batch)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            for k, v in metrics.items():\n",
        "                accumulators[k] = accumulators.get(k, 0.0) + v\n",
        "\n",
        "            n_batches += 1\n",
        "\n",
        "        if n_batches == 0:\n",
        "            return 0.0, {}\n",
        "\n",
        "        avg_metrics = {k: v / n_batches for k, v in accumulators.items()}\n",
        "        return total_loss / n_batches, avg_metrics\n",
        "\n",
        "    def _save_history(self):\n",
        "        save_dict = {k: np.array(v) for k, v in self.history.items()}\n",
        "        np.savez_compressed(\n",
        "            os.path.join(self.save_dir, \"loss_history.npz\"),\n",
        "            **save_dict,\n",
        "        )\n",
        "\n",
        "    def _plot_losses(self):\n",
        "        epochs = np.arange(1, len(self.history[\"train_total\"]) + 1)\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Plot 1: Lambda Max\n",
        "        ax1.plot(\n",
        "            epochs,\n",
        "            self.history[\"train_lambda\"],\n",
        "            label=\"Train Lambda\",\n",
        "            color=\"tab:purple\",\n",
        "        )\n",
        "        ax1.plot(\n",
        "            epochs,\n",
        "            self.history[\"val_lambda\"],\n",
        "            \"--\",\n",
        "            label=\"Val Lambda\",\n",
        "            color=\"tab:purple\",\n",
        "        )\n",
        "        ax1.set_title(\"Absorption (Lambda) Loss\")\n",
        "        ax1.set_xlabel(\"Epoch\")\n",
        "        ax1.set_ylabel(\"Huber Loss\")\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot 2: Phi\n",
        "        ax2.plot(\n",
        "            epochs, self.history[\"train_phi\"], label=\"Train Phi\", color=\"tab:brown\"\n",
        "        )\n",
        "        ax2.plot(\n",
        "            epochs, self.history[\"val_phi\"], \"--\", label=\"Val Phi\", color=\"tab:brown\"\n",
        "        )\n",
        "        ax2.set_title(\"Quantum Yield (Phi) Loss\")\n",
        "        ax2.set_xlabel(\"Epoch\")\n",
        "        ax2.set_ylabel(\"MSE Loss\")\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        fig_path = os.path.join(self.save_dir, \"loss_curves.png\")\n",
        "        plt.savefig(fig_path)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "print(\"‚úì Training engine loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Phase 1: Train Lambda Head (Pre-training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 1: LAMBDA HEAD TRAINING (Pre-training)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 1: TRAINING LAMBDA HEAD\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check dataset exists\n",
        "if not LAMBDA_DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Lambda dataset not found at {LAMBDA_DATASET_PATH}\")\n",
        "\n",
        "print(f\"Dataset: {LAMBDA_DATASET_PATH}\")\n",
        "print(f\"Save Dir: {SAVE_DIR_LAMBDA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Lambda Dataset\n",
        "lambda_dataset = DatasetConstructor(\n",
        "    str(LAMBDA_DATASET_PATH),\n",
        "    cutoff_radius=5.0,\n",
        "    batch_size=64,\n",
        "    train_split=0.8,\n",
        "    val_split=0.1,\n",
        "    random_seed=42,\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = lambda_dataset.get_dataloaders(num_workers=0)\n",
        "\n",
        "print(f\"Atom types: {lambda_dataset.num_atom_types}\")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lambda Hyperparameters\n",
        "lambda_hyperparams = {\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"max_epochs\": 300,\n",
        "    \"early_stopping_patience\": 60,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"tasks\": [\"lambda\"],\n",
        "}\n",
        "\n",
        "# Create Lambda Model\n",
        "lambda_model = MANA(\n",
        "    num_atom_types=lambda_dataset.num_atom_types,\n",
        "    hidden_dim=128,\n",
        "    num_layers=4,\n",
        "    num_rbf=20,\n",
        "    tasks=lambda_hyperparams[\"tasks\"],\n",
        "    lambda_mean=lambda_dataset.lambda_mean,\n",
        "    lambda_std=lambda_dataset.lambda_std,\n",
        ")\n",
        "\n",
        "# Device Selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lambda_model = lambda_model.to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Learning rate: {lambda_hyperparams['learning_rate']}\")\n",
        "print(f\"Max epochs: {lambda_hyperparams['max_epochs']}\")\n",
        "print(f\"Weight decay: {lambda_hyperparams['weight_decay']}\")\n",
        "print(f\"Active Training Tasks: {lambda_hyperparams['tasks']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_params = sum(p.numel() for p in lambda_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in lambda_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model statistics:\")\n",
        "print(f\"  Total parameters:     {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Lambda Model\n",
        "lambda_engine = TrainingEngine(\n",
        "    model=lambda_model,\n",
        "    device=device,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    hyperparams=lambda_hyperparams,\n",
        "    save_dir=str(SAVE_DIR_LAMBDA),\n",
        ")\n",
        "\n",
        "try:\n",
        "    lambda_engine.train()\n",
        "    print(\"\\n‚úì Lambda training completed successfully!\")\n",
        "    print(f\"  Model saved to: {SAVE_DIR_LAMBDA / 'best_model.pth'}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nTraining failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Phase 2: Train Phi Head (Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: PHI HEAD TRAINING (Fine-tuning with pretrained backbone)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 2: TRAINING PHI HEAD (with pretrained backbone)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check dataset exists\n",
        "if not PHI_DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Phi dataset not found at {PHI_DATASET_PATH}\")\n",
        "\n",
        "print(f\"Dataset: {PHI_DATASET_PATH}\")\n",
        "print(f\"Save Dir: {SAVE_DIR_PHI}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Phi Dataset\n",
        "phi_dataset = DatasetConstructor(\n",
        "    str(PHI_DATASET_PATH),\n",
        "    cutoff_radius=5.0,\n",
        "    batch_size=64,\n",
        "    train_split=0.8,\n",
        "    val_split=0.1,\n",
        "    random_seed=42,\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = phi_dataset.get_dataloaders(num_workers=0)\n",
        "\n",
        "print(f\"Atom types: {phi_dataset.num_atom_types}\")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phi Hyperparameters\n",
        "phi_hyperparams = {\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"max_epochs\": 250,\n",
        "    \"early_stopping_patience\": 40,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"tasks\": [\"phi\"],\n",
        "}\n",
        "\n",
        "# Create Phi Model\n",
        "phi_model = MANA(\n",
        "    num_atom_types=phi_dataset.num_atom_types,\n",
        "    hidden_dim=128,\n",
        "    num_layers=4,\n",
        "    num_rbf=20,\n",
        "    tasks=phi_hyperparams[\"tasks\"],\n",
        "    lambda_mean=phi_dataset.lambda_mean,\n",
        "    lambda_std=phi_dataset.lambda_std,\n",
        ")\n",
        "\n",
        "# Load pretrained backbone from lambda training\n",
        "pretrained_path = SAVE_DIR_LAMBDA / \"best_model.pth\"\n",
        "if pretrained_path.exists():\n",
        "    print(f\"Loading pretrained weights from: {pretrained_path}\")\n",
        "    phi_model.load_state_dict(torch.load(pretrained_path), strict=False)\n",
        "    print(\"‚úì Pretrained backbone loaded (strict=False for task head mismatch)\")\n",
        "else:\n",
        "    print(\"‚ö† No pretrained model found - training from scratch\")\n",
        "\n",
        "# Device Selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "phi_model = phi_model.to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Learning rate: {phi_hyperparams['learning_rate']}\")\n",
        "print(f\"Max epochs: {phi_hyperparams['max_epochs']}\")\n",
        "print(f\"Weight decay: {phi_hyperparams['weight_decay']}\")\n",
        "print(f\"Active Training Tasks: {phi_hyperparams['tasks']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_params = sum(p.numel() for p in phi_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in phi_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model statistics:\")\n",
        "print(f\"  Total parameters:     {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Phi Model\n",
        "phi_engine = TrainingEngine(\n",
        "    model=phi_model,\n",
        "    device=device,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    hyperparams=phi_hyperparams,\n",
        "    save_dir=str(SAVE_DIR_PHI),\n",
        ")\n",
        "\n",
        "try:\n",
        "    phi_engine.train()\n",
        "    print(\"\\n‚úì Phi training completed successfully!\")\n",
        "    print(f\"  Model saved to: {SAVE_DIR_PHI / 'best_model.pth'}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nTraining failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Summary & Saved Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Training Complete - Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nSaved artifacts in Google Drive:\")\n",
        "print(f\"\\nüìÅ {SAVE_DIR_LAMBDA}\")\n",
        "for f in SAVE_DIR_LAMBDA.glob(\"*\"):\n",
        "    print(f\"   ‚îî‚îÄ‚îÄ {f.name}\")\n",
        "\n",
        "print(f\"\\nüìÅ {SAVE_DIR_PHI}\")\n",
        "for f in SAVE_DIR_PHI.glob(\"*\"):\n",
        "    print(f\"   ‚îî‚îÄ‚îÄ {f.name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"You can now use the trained models for inference!\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display loss curves if they exist\n",
        "from IPython.display import Image, display\n",
        "\n",
        "lambda_curves = SAVE_DIR_LAMBDA / \"loss_curves.png\"\n",
        "phi_curves = SAVE_DIR_PHI / \"loss_curves.png\"\n",
        "\n",
        "if lambda_curves.exists():\n",
        "    print(\"Lambda Training Loss Curves:\")\n",
        "    display(Image(filename=str(lambda_curves)))\n",
        "\n",
        "if phi_curves.exists():\n",
        "    print(\"\\nPhi Training Loss Curves:\")\n",
        "    display(Image(filename=str(phi_curves)))"
      ]
    }
  ]
}
